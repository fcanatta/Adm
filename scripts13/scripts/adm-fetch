#!/usr/bin/env bash
# adm-fetch - download de múltiplos sources em paralelo, com checagem de soma

set -o pipefail
set -o nounset

# =========[ CONFIG BÁSICA ]================================================

ADM_ROOT=${ADM_ROOT:-/usr/src/adm}
ADM_REPO="${ADM_ROOT}/repo"
ADM_CACHE="${ADM_ROOT}/cache"
ADM_LOG_DIR="${ADM_ROOT}/logs"

DRY_RUN=0
QUIET=0
MAX_JOBS=${MAX_JOBS:-4}   # downloads paralelos máximos

# =========[ CORES / LOG ]==================================================

C_RESET='\033[0m'
C_BOLD='\033[1m'

C_MAGENTA='\033[35;1m'
C_GREEN='\033[32;1m'
C_YELLOW='\033[33;1m'
C_RED='\033[31;1m'
C_CYAN='\033[36;1m'

CHECK_MARK="✔"

supports_color() {
    [[ -t 1 ]] && tput colors &>/dev/null
}

colorize() {
    local color="$1"; shift
    if supports_color; then
        printf "%b%s%b" "${color}" "$*" "${C_RESET}"
    else
        printf "%s" "$*"
    fi
}

timestamp() {
    date +%H:%M:%S
}

log_header() {
    local msg="$*"
    (( QUIET )) && return 0
    printf "%b %s %b%s%b\n" \
        "$(colorize "${C_MAGENTA}" "==")" \
        "$(timestamp)" \
        "$(colorize "${C_MAGENTA}" "[FETCH]")" \
        " ${msg}" \
        "${C_RESET}"
}

log_arrow() {
    local msg="$*"
    (( QUIET )) && return 0
    printf "%b %s %s\n" \
        "$(colorize "${C_GREEN}" "->")" \
        "$(timestamp)" \
        "${msg}"
}

log_info() {
    local msg="$*"
    (( QUIET )) && return 0
    printf "%b %s %s\n" \
        "$(colorize "${C_CYAN}" "->")" \
        "$(timestamp)" \
        "${msg}"
}

log_warn() {
    local msg="$*"
    printf "%b %s %s\n" \
        "$(colorize "${C_YELLOW}" "!!")" \
        "$(timestamp)" \
        "${msg}" 1>&2
}

log_error() {
    local msg="$*"
    printf "%b %s %s\n" \
        "$(colorize "${C_RED}" "!!")" \
        "$(timestamp)" \
        "${msg}" 1>&2
}

die() {
    log_error "$*"
    exit 1
}

# =========[ SPINNER ]======================================================

_spinner_pid=""

spinner_start() {
    local msg="$1"
    (( QUIET )) && return 0

    local spin='|/-\'
    local i=0

    printf "%s %s %s " "$(colorize "${C_GREEN}" "->")" "$(timestamp)" "${msg}"

    (
        tput civis 2>/dev/null || true
        while :; do
            printf "\r%s %s %s %b%c%b" \
                "$(colorize "${C_GREEN}" "->")" \
                "$(timestamp)" \
                "${msg}" \
                "${C_CYAN}" "${spin:i++%${#spin}:1}" "${C_RESET}"
            sleep 0.1
        done
    ) &
    _spinner_pid=$!
    disown "$_spinner_pid" 2>/dev/null || true
}

spinner_stop() {
    local status=${1:-0}
    (( QUIET )) && return "${status}"

    if [[ -n "${_spinner_pid}" ]] && kill -0 "${_spinner_pid}" &>/dev/null; then
        kill "${_spinner_pid}" &>/dev/null || true
        wait "${_spinner_pid}" 2>/dev/null || true
    fi
    _spinner_pid=""

    tput cnorm 2>/dev/null || true

    local symbol msg_color
    if (( status == 0 )); then
        symbol="${CHECK_MARK}"
        msg_color="${C_GREEN}"
    else
        symbol="✖"
        msg_color="${C_RED}"
    fi

    printf "\r%b %s %b%s%b\n" \
        "$(colorize "${msg_color}" "->")" \
        "$(timestamp)" \
        "${msg_color}" "[${symbol}]" "${C_RESET}"

    return "${status}"
}

run_with_spinner() {
    local msg="$1"; shift

    if (( DRY_RUN )); then
        log_info "[dry-run] ${msg}: $*"
        return 0
    fi

    spinner_start "${msg}"
    "$@" &
    local pid=$!

    wait "${pid}"
    local status=$?

    spinner_stop "${status}"
    return "${status}"
}

# =========[ DIRETÓRIOS / LOG ]=============================================

ensure_dirs() {
    local d
    for d in "${ADM_CACHE}" "${ADM_LOG_DIR}"; do
        if [[ ! -d "${d}" ]]; then
            if (( DRY_RUN )); then
                log_arrow "[dry-run] mkdir -p ${d}"
            else
                mkdir -p "${d}" || die "Falha ao criar diretório ${d}"
            fi
        fi
    done
}

log_file_for() {
    local category="$1" pkg="$2"
    printf "%s/fetch-%s-%s.%s.log" "${ADM_LOG_DIR}" "${category}" "${pkg}" "$(date +%Y%m%d-%H%M%S)"
}

# =========[ METAFILE ]=====================================================

meta_path() {
    local category="$1" pkg="$2"
    printf "%s/%s/%s/metafile" "${ADM_REPO}" "${category}" "${pkg}"
}

meta_get() {
    local category="$1" pkg="$2" key="$3"
    local file
    file="$(meta_path "${category}" "${pkg}")"
    [[ -f "${file}" ]] || die "Metafile não encontrado: ${file}"

    local line
    line="$(grep -E "^${key}=" "${file}" 2>/dev/null || true)"
    [[ -z "${line}" ]] && return 1
    echo "${line#${key}=}"
}

meta_name()        { meta_get "$1" "$2" "name";        }
meta_version()     { meta_get "$1" "$2" "version";     }
meta_sources()     { meta_get "$1" "$2" "sources";     }
meta_sha256sums()  { meta_get "$1" "$2" "sha256sums";  }
meta_md5sums()     { meta_get "$1" "$2" "md5sum";      }

# =========[ ENCONTRAR METAFILE QUANDO SÓ TEM PROGRAMA ]====================

find_category_for_pkg() {
    local pkg="$1"

    local matches=()
    local f
    while IFS= read -r -d '' f; do
        # .../repo/<category>/<pkg>/metafile
        local cat
        cat="$(basename "$(dirname "${f}")")"
        local dirpkg
        dirpkg="$(basename "$(dirname "$(dirname "${f}")")")"
        if [[ "${dirpkg}" == "${pkg}" ]]; then
            matches+=("${cat}")
        fi
    done < <(find "${ADM_REPO}" -mindepth 3 -maxdepth 5 -type f -name 'metafile' -print0 2>/dev/null || true)

    if (( ${#matches[@]} == 0 )); then
        die "Nenhum metafile encontrado para programa '${pkg}'."
    elif (( ${#matches[@]} > 1 )); then
        log_error "Metafiles múltiplos encontrados para '${pkg}':"
        local m
        for m in "${matches[@]}"; do
            echo "  - ${m}/${pkg}"
        done
        die "Especifique a categoria explicitamente."
    else
        echo "${matches[0]}"
    fi
}

# =========[ PARSE LISTAS ]=================================================

split_list() {
    # converte "a,b,c" -> linhas
    local s="$1"
    [[ -z "${s}" ]] && return 0
    echo "${s}" | tr ',' '\n' | sed '/^[[:space:]]*$/d'
}

# =========[ DETECÇÃO DE TIPO DE SOURCE ]===================================

detect_scheme() {
    local url="$1"

    case "${url}" in
        git+http://*|git+https://*|git+ssh://*|git://*)
            echo "git"
            ;;
        http://*|https://*)
            # podemos decidir git x http pelo sufixo
            if [[ "${url}" == *.git ]]; then
                echo "git"
            else
                echo "http"
            fi
            ;;
        rsync://*)
            echo "rsync"
            ;;
        ftp://*|ftps://*)
            echo "ftp"
            ;;
        file://*)
            echo "file"
            ;;
        /*|./*|../*)
            # caminho local
            if [[ -d "${url}" ]]; then
                echo "dir"
            else
                echo "file"
            fi
            ;;
        *)
            # github / gitlab / sourceforge sem esquema explícito -> assume https
            if [[ "${url}" == *github.com* || "${url}" == *gitlab.com* || "${url}" == *sourceforge.net* ]]; then
                echo "http"
            else
                echo "http"
            fi
            ;;
    esac
}

# =========[ FERRAMENTAS DE DOWNLOAD ]======================================

have_cmd() { command -v "$1" &>/dev/null; }

dl_http() {
    local url="$1" out="$2" log_file="$3"

    if (( DRY_RUN )); then
        log_info "[dry-run] baixar ${url} -> ${out}"
        return 0
    fi

    mkdir -p "$(dirname "${out}")" || die "Falha ao criar diretório de saída."

    {
        echo "=== $(date) ==="
        echo "HTTP: ${url}"
    } >> "${log_file}"

    if have_cmd curl; then
        curl -L --fail -o "${out}" "${url}" >> "${log_file}" 2>&1 || {
            rm -f "${out}" 2>/dev/null || true
            die "Falha no download via curl: ${url}"
        }
    elif have_cmd wget; then
        wget -O "${out}" "${url}" >> "${log_file}" 2>&1 || {
            rm -f "${out}" 2>/dev/null || true
            die "Falha no download via wget: ${url}"
        }
    else
        die "Nenhum downloader HTTP disponível (precisa de curl ou wget)."
    fi
}

dl_rsync() {
    local url="$1" outdir="$2" log_file="$3"

    if (( DRY_RUN )); then
        log_info "[dry-run] rsync -av ${url} ${outdir}/"
        return 0
    fi

    have_cmd rsync || die "rsync não encontrado."

    mkdir -p "${outdir}" || die "Falha ao criar ${outdir}"

    {
        echo "=== $(date) ==="
        echo "RSYNC: ${url}"
    } >> "${log_file}"

    rsync -av "${url}" "${outdir}/" >> "${log_file}" 2>&1 || die "Falha no rsync: ${url}"
}

dl_git_to_tar() {
    local url="$1" out_tar="$2" log_file="$3"

    if (( DRY_RUN )); then
        log_info "[dry-run] git clone --depth 1 ${url} TMP && tar -cf ${out_tar} ..."
        return 0
    fi

    have_cmd git || die "git não encontrado."

    mkdir -p "$(dirname "${out_tar}")" || die "Falha ao criar dir do tar."

    local tmpdir
    tmpdir="$(mktemp -d "${ADM_CACHE}/git-tmp-XXXXXX")" || die "Falha ao criar tmpdir git."

    {
        echo "=== $(date) ==="
        echo "GIT: ${url}"
        echo "TMP: ${tmpdir}"
    } >> "${log_file}"

    if [[ "${url}" == git+* ]]; then
        url="${url#git+}"
    fi

    git clone --depth 1 "${url}" "${tmpdir}/src" >> "${log_file}" 2>&1 || {
        rm -rf "${tmpdir}" || true
        die "Falha no git clone: ${url}"
    }

    # cria tar.xz para economizar espaço
    if have_cmd xz; then
        tar -C "${tmpdir}/src" -cf - . | xz -9 -T0 -c > "${out_tar}" 2>> "${log_file}" || {
            rm -rf "${tmpdir}" || true
            rm -f "${out_tar}" || true
            die "Falha ao criar tar.xz de git: ${url}"
        }
    else
        tar -C "${tmpdir}/src" -cf "${out_tar}" . >> "${log_file}" 2>&1 || {
            rm -rf "${tmpdir}" || true
            rm -f "${out_tar}" || true
            die "Falha ao criar tar de git: ${url}"
        }
    fi

    rm -rf "${tmpdir}" || true
}

tar_from_dir() {
    local srcdir="$1" out_tar="$2" log_file="$3"

    if (( DRY_RUN )); then
        log_info "[dry-run] tar -cf ${out_tar} -C ${srcdir} . (possivelmente com xz)"
        return 0
    fi

    [[ -d "${srcdir}" ]] || die "Diretório local não existe: ${srcdir}"
    mkdir -p "$(dirname "${out_tar}")" || die "Falha ao criar diretório de saída."

    {
        echo "=== $(date) ==="
        echo "DIR->TAR: ${srcdir}"
    } >> "${log_file}"

    if have_cmd xz; then
        tar -C "${srcdir}" -cf - . | xz -9 -T0 -c > "${out_tar}" 2>> "${log_file}" || {
            rm -f "${out_tar}" || true
            die "Falha ao criar tar.xz de diretório: ${srcdir}"
        }
    else
        tar -C "${srcdir}" -cf "${out_tar}" . >> "${log_file}" 2>&1 || {
            rm -f "${out_tar}" || true
            die "Falha ao criar tar de diretório: ${srcdir}"
        }
    fi
}

copy_local_file() {
    local src="$1" dest="$2" log_file="$3"

    if (( DRY_RUN )); then
        log_info "[dry-run] cp ${src} ${dest}"
        return 0
    fi

    [[ -f "${src}" ]] || die "Arquivo local não encontrado: ${src}"
    mkdir -p "$(dirname "${dest}")" || die "Falha ao criar diretório de destino."

    {
        echo "=== $(date) ==="
        echo "LOCAL: ${src}"
    } >> "${log_file}"

    cp -f "${src}" "${dest}" >> "${log_file}" 2>&1 || die "Falha ao copiar arquivo local: ${src}"
}

# =========[ CHECAGEM DE SOMA ]=============================================

check_sha256() {
    local file="$1" expected="$2"

    [[ -z "${expected}" ]] && return 0
    [[ -f "${file}" ]] || die "Arquivo para sha256 não existe: ${file}"

    if ! have_cmd sha256sum; then
        log_warn "sha256sum não encontrado; ignorando checagem de sha256."
        return 0
    fi

    local got
    got="$(sha256sum "${file}" | awk '{print $1}')"
    if [[ "${got}" != "${expected}" ]]; then
        rm -f "${file}" 2>/dev/null || true
        die "sha256sum incorreto para ${file}: esperado=${expected}, obtido=${got}"
    fi
}

check_md5() {
    local file="$1" expected="$2"

    [[ -z "${expected}" ]] && return 0
    [[ -f "${file}" ]] || die "Arquivo para md5 não existe: ${file}"

    if ! have_cmd md5sum; then
        log_warn "md5sum não encontrado; ignorando checagem de md5."
        return 0
    fi

    local got
    got="$(md5sum "${file}" | awk '{print $1}')"
    if [[ "${got}" != "${expected}" ]]; then
        rm -f "${file}" 2>/dev/null || true
        die "md5sum incorreto para ${file}: esperado=${expected}, obtido=${got}"
    fi
}
# =========[ DOWNLOAD DE UM SOURCE (UM ÍNDICE) ]============================

download_one_source() {
    local idx="$1" url="$2" name="$3" version="$4" sha256="$5" md5="$6" log_file="$7"

    ensure_dirs

    local scheme out filebase
    scheme="$(detect_scheme "${url}")"

    # Nome base para arquivos que a gente gerar; garantimos que contenha name-version
    filebase="${name}-${version}-src${idx}"

    case "${scheme}" in
        http|ftp)
            local base
            base="$(basename "${url}")"
            [[ -z "${base}" || "${base}" == "/" ]] && base="${filebase}.tar"
            out="${ADM_CACHE}/${base}"
            ;;

        git)
            # sempre gera um tar no cache
            out="${ADM_CACHE}/${filebase}.tar.xz"
            ;;

        rsync)
            # rsync geralmente copia diretórios; geramos um tar.xz após sync
            out="${ADM_CACHE}/${filebase}.tar.xz"
            ;;

        file)
            local path
            path="${url#file://}"
            out="${ADM_CACHE}/$(basename "${path}")"
            ;;

        dir)
            out="${ADM_CACHE}/${filebase}.tar.xz"
            ;;

        *)
            # fallback: trata como http
            local base
            base="$(basename "${url}")"
            [[ -z "${base}" || "${base}" == "/" ]] && base="${filebase}.tar"
            out="${ADM_CACHE}/${base}"
            ;;
    esac

    # Se já existe e checksum bate, reutiliza
    if [[ -f "${out}" && -z "${sha256}" && -z "${md5}" ]]; then
        log_arrow "[#${idx}] Arquivo já no cache (sem checagem): $(basename "${out}")"
        echo "${out}"
        return 0
    fi

    if [[ -f "${out}" && -n "${sha256}" ]]; then
        if check_sha256 "${out}" "${sha256}"; then
            log_arrow "[#${idx}] Arquivo já no cache (sha256 OK): $(basename "${out}")"
            echo "${out}"
            return 0
        fi
    elif [[ -f "${out}" && -n "${md5}" ]]; then
        if check_md5 "${out}" "${md5}"; then
            log_arrow "[#${idx}] Arquivo já no cache (md5 OK): $(basename "${out}")"
            echo "${out}"
            return 0
        fi
    fi

    log_arrow "[#${idx}] Baixando: ${url} -> $(basename "${out}") (tipo=${scheme})"

    case "${scheme}" in
        http)
            dl_http "${url}" "${out}" "${log_file}"
            ;;

        ftp)
            dl_http "${url}" "${out}" "${log_file}"
            ;;

        git)
            dl_git_to_tar "${url}" "${out}" "${log_file}"
            ;;

        rsync)
            # baixa para tmpdir e tar
            local tmpdir
            tmpdir="$(mktemp -d "${ADM_CACHE}/rsync-tmp-XXXXXX")" || die "Falha ao criar tmpdir para rsync."
            dl_rsync "${url}" "${tmpdir}" "${log_file}"
            tar_from_dir "${tmpdir}" "${out}" "${log_file}"
            rm -rf "${tmpdir}" || true
            ;;

        file)
            local path
            path="${url#file://}"
            copy_local_file "${path}" "${out}" "${log_file}"
            ;;

        dir)
            tar_from_dir "${url}" "${out}" "${log_file}"
            ;;

        *)
            dl_http "${url}" "${out}" "${log_file}"
            ;;
    esac

    # Checagem de soma, se fornecida
    if [[ -n "${sha256}" ]]; then
        check_sha256 "${out}" "${sha256}"
    elif [[ -n "${md5}" ]]; then
        check_md5 "${out}" "${md5}"
    fi

    echo "${out}"
}

# =========[ DOWNLOAD DE TODOS OS SOURCES (PARALELO) ]======================

download_all_sources() {
    local name="$1" version="$2" log_file="$3"; shift 3
    local -a urls=("$@")

    local -a pids=()
    local -a files=()
    local -a idxs=()

    local total="${#urls[@]}"

    # arrays de soma por índice (variáveis globais SHA_LIST / MD5_LIST)
    local i
    for (( i=0; i<total; i++ )); do
        :
    done

    # closure pro bash: usa variáveis globais SHA_LIST/MD5_LIST
    _download_all_worker() {
        local idx="$1" url="$2" name="$3" version="$4" sha="$5" md="$6" log_file="$7" outvar="$8"

        local out
        out="$(download_one_source "${idx}" "${url}" "${name}" "${version}" "${sha}" "${md}" "${log_file}")" || return 1
        printf "%s\n" "${out}" > "${outvar}"
    }

    local running=0
    local tmpfiles=()

    for (( i=0; i<total; i++ )); do
        local idx=$(( i + 1 ))
        local url="${urls[i]}"
        local sha="${SHA_LIST[i]:-}"
        local md="${MD5_LIST[i]:-}"

        local tmpfile
        tmpfile="$(mktemp "${ADM_CACHE}/.fetch-${name}-${version}-src${idx}.XXXX")"
        tmpfiles+=("${tmpfile}")
        idxs+=("${idx}")

        _download_all_worker "${idx}" "${url}" "${name}" "${version}" "${sha}" "${md}" "${log_file}" "${tmpfile}" &
        pids+=("$!")
        running=$(( running + 1 ))

        # limita jobs
        if (( running >= MAX_JOBS )); then
            wait -n || return 1
            running=$(( running - 1 ))
        fi
    done

    # espera todo mundo
    local pid
    for pid in "${pids[@]}"; do
        wait "${pid}" || return 1
    done

    # lê resultados
    local tf out
    for tf in "${tmpfiles[@]}"; do
        if [[ -f "${tf}" ]]; then
            out="$(cat "${tf}")"
            files+=("${out}")
            rm -f "${tf}" || true
        fi
    done

    # imprime lista final (um por linha)
    printf "%s\n" "${files[@]}"
}

# =========[ COMANDO FETCH (METAFILE) ]=====================================

cmd_fetch() {
    local category="" pkg=""

    if [[ $# -eq 1 ]]; then
        pkg="$1"
        category="$(find_category_for_pkg "${pkg}")"
    elif [[ $# -eq 2 ]]; then
        category="$1"
        pkg="$2"
    else
        die "Uso: adm-fetch fetch <programa> OU adm-fetch fetch <categoria> <programa>"
    fi

    ensure_dirs

    local name version sources sha256s md5s
    name="$(meta_name "${category}" "${pkg}")"
    version="$(meta_version "${category}" "${pkg}")"
    sources="$(meta_sources "${category}" "${pkg}")"
    sha256s="$(meta_sha256sums "${category}" "${pkg}" 2>/dev/null || echo "")"
    md5s="$(meta_md5sums "${category}" "${pkg}" 2>/dev/null || echo "")"

    local log_file
    log_file="$(log_file_for "${category}" "${pkg}")"

    log_header "Download de sources para ${category}/${pkg}"
    log_info "Name:    ${name}"
    log_info "Version: ${version}"
    log_info "Log:     ${log_file}"

    local -a URL_LIST=()
    local -a SHA_ARR=()
    local -a MD5_ARR=()

    local s
    while IFS= read -r s; do
        [[ -z "${s}" ]] && continue
        URL_LIST+=("${s}")
    done < <(split_list "${sources}")

    local ucount="${#URL_LIST[@]}"
    if (( ucount == 0 )); then
        die "Metafile de ${category}/${pkg} não contém 'sources=' válidos."
    fi

    local sha
    while IFS= read -r sha; do
        SHA_ARR+=("${sha}")
    done < <(split_list "${sha256s}")

    local md
    while IFS= read -r md; do
        MD5_ARR+=("${md}")
    done < <(split_list "${md5s}")

    # torna acessíveis globalmente para download_all_sources
    SHA_LIST=()
    MD5_LIST=()
    local i
    for (( i=0; i<ucount; i++ )); do
        SHA_LIST[i]="${SHA_ARR[i]:-}"
        MD5_LIST[i]="${MD5_ARR[i]:-}"
    done

    export SHA_LIST MD5_LIST

    # função que será chamada com spinner
    _run_download_all() {
        download_all_sources "${name}" "${version}" "${log_file}" "${URL_LIST[@]}"
    }

    if (( DRY_RUN )); then
        log_info "[dry-run] baixaria ${ucount} sources para ${name}-${version}."
        local j
        for (( j=0; j<ucount; j++ )); do
            log_info "[dry-run] [#${j}] ${URL_LIST[j]}"
        done
        return 0
    fi

    run_with_spinner "Baixando ${ucount} sources" _run_download_all || die "Falha no download de algum source."

    log_arrow "Download de sources concluído para ${category}/${pkg}."
}

# =========[ COMANDO URL (baixar URLs arbitrárias) ]========================

cmd_url() {
    ensure_dirs

    local urls=()
    while [[ $# -gt 0 ]]; do
        urls+=("$1"); shift
    done

    [[ ${#urls[@]} -ge 1 ]] || die "Uso: adm-fetch url <url1> [url2 ...]"

    log_header "Download de URLs avulsas para ${ADM_CACHE}"

    local i=0
    local name="manual"
    local version="1"

    # sem checksum
    SHA_LIST=()
    MD5_LIST=()

    local log_file="${ADM_LOG_DIR}/fetch-manual.$(date +%Y%m%d-%H%M%S).log"

    _run_download_all_url() {
        download_all_sources "${name}" "${version}" "${log_file}" "${urls[@]}"
    }

    if (( DRY_RUN )); then
        log_info "[dry-run] baixaria ${#urls[@]} URLs para ${ADM_CACHE}"
        local u
        for u in "${urls[@]}"; do
            log_info "[dry-run] ${u}"
        done
        return 0
    fi

    run_with_spinner "Baixando URLs avulsas" _run_download_all_url || die "Falha ao baixar URLs."

    log_arrow "Download de URLs avulsas concluído."
}

# =========[ HELP / PARSE GLOBAL / MAIN ]===================================

cmd_help() {
    cat <<EOF
Uso: adm-fetch [OPÇÕES] <comando> [args]

OPÇÕES:
  -n, --dry-run      Não executa, apenas mostra o que faria
  -q, --quiet        Menos saída (apenas erros)
  -h, --help         Mostra esta ajuda

COMANDOS:
  fetch <programa>
      Usa o metafile encontrado em /usr/src/adm/repo/*/<programa>/metafile
      (se houver mais de uma categoria, erro) e baixa todos os sources.

  fetch <categoria> <programa>
      Usa o metafile explícito /usr/src/adm/repo/<categoria>/<programa>/metafile.

  url <url1> [url2 ...]
      Baixa URLs arbitrárias para o cache /usr/src/adm/cache (sem metafile).

Notas:
  - Downloads podem ser http/https, git, rsync, ftp, file:// e diretórios locais.
  - Para git e diretórios, um tar (normalmente tar.xz) é criado no cache,
    contendo o conteúdo do repositório/diretório.
  - sha256sums= e md5sum= do metafile são usados para checagem se presentes.
EOF
}

parse_global_opts() {
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -n|--dry-run)
                DRY_RUN=1; shift ;;
            -q|--quiet)
                QUIET=1; shift ;;
            -h|--help)
                cmd_help; exit 0 ;;
            --)
                shift; break ;;
            -*)
                die "Opção desconhecida: $1" ;;
            *)
                break ;;
        esac
    done

    echo "$#"
}

main() {
    local argc
    argc=$(parse_global_opts "$@")
    local args=("$@")
    local consumed=$(( ${#args[@]} - argc ))
    args=("${args[@]:${consumed}}")

    if [[ ${#args[@]} -lt 1 ]]; then
        cmd_help
        exit 1
    fi

    local cmd="${args[0]}"
    shift || true

    case "${cmd}" in
        fetch)  cmd_fetch "$@" ;;
        url)    cmd_url   "$@" ;;
        help|-h|--help) cmd_help ;;
        *)
            die "Comando desconhecido: ${cmd}"
            ;;
    esac
}

trap 'spinner_stop 1 >/dev/null 2>&1 || true' INT TERM

main "$@"
