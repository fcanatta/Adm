#!/usr/bin/env bash
# ================================================================
# adm-fetch v1.0 -
# Advanced fetcher For ADM
# - curl -> wget fallback
# - robust git ref parsing
# - spinner & progress helpers
# - safe temp dirs, logging, traps
# 
# ================================================================

set -euo pipefail
IFS=$'\n\t'

# ---------------------------
# CONFIGURAÇÃO PADRÃO
# ---------------------------
ADM_ROOT=${ADM_ROOT:-/usr/src/adm}
CFG_DIR="$ADM_ROOT/cfg"
SRC_DIR="$ADM_ROOT/src"
LOG_DIR="$ADM_ROOT/logs"
HOOK_DIR="$ADM_ROOT/hooks"
TMP_DIR="${TMP_DIR:-$ADM_ROOT/tmp}"
PROFILE_DIR="$CFG_DIR/profiles"

SOURCES_LIST="${SOURCES_LIST:-$CFG_DIR/sources.list}"
DELIM="${DELIM:-|}"

# Timeouts & retries
NET_RETRIES=3
NET_RETRY_DELAY=3
GIT_RETRIES=2
GIT_RETRY_DELAY=5

# Ensure dirs
mkdir -p "$CFG_DIR" "$SRC_DIR" "$LOG_DIR" "$HOOK_DIR/pre/fetch" "$HOOK_DIR/post/fetch" "$TMP_DIR" "$PROFILE_DIR"

# ---------------------------
# LOGGING (global + per-package)
# ---------------------------
GLOBAL_LOG="$LOG_DIR/fetch.log"
: > "$GLOBAL_LOG" 2>&1

# All stdout/stderr should still be available to terminal for the short lines.
# We'll write verbose outputs to individual package logs.

# ---------------------------
# CORES E FORMATACAO
# ---------------------------
CLR_RESET='\e[0m'
CLR_MAGENTA='\e[95m'
CLR_GREEN='\e[92m'
CLR_CYAN='\e[96m'
CLR_YELLOW='\e[93m'
CLR_RED='\e[91m'

# small helper for timestamped short lines
timestamp(){ date '+%H:%M:%S'; }

headline() {
    # magenta header with timestamp
    printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$1"
}

short_ok(){ printf "%b[✔]%b %s\n" "$CLR_GREEN" "$CLR_RESET" "$1"; }
short_fail(){ printf "%b[✖]%b %s\n" "$CLR_RED" "$CLR_RESET" "$1"; }
short_info(){ printf "%b[%s]%b %s\n" "$CLR_GREEN" "$(timestamp)" "$CLR_RESET" "$1"; }

# ---------------------------
# SPINNERS & PROGRESS STYLES
# - curl spinner (dots style)
# - wget spinner (bar-ish minimal)
# ---------------------------

# spinner process handle
__spinner_pid=""

# generic spinner loop (unicode frames)
__spinner_frames=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏')
start_spinner() {
    # args: message
    local msg="${1:-working...}"
    # print header line then start spinner on new line
    printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$msg"
    (
        local i=0
        while :; do
            printf "\r%b %s %b" "$CLR_GREEN" "${__spinner_frames[i % ${#__spinner_frames[@]}]}" "$CLR_RESET"
            printf " %s" "$msg"
            i=$((i+1))
            sleep 0.12
        done
    ) &
    __spinner_pid=$!
    disown "$__spinner_pid" 2>/dev/null || true
}

stop_spinner() {
    # args: exit_code, final_message (optional)
    local code=${1:-0}
    local final_msg="${2:-Concluído.}"
    if [[ -n "$__spinner_pid" ]]; then
        kill "$__spinner_pid" >/dev/null 2>&1 || true
        wait "$__spinner_pid" 2>/dev/null || true
        __spinner_pid=""
    fi
    if [[ "$code" -eq 0 ]]; then
        printf "\r%b[✔]%b %s\n" "$CLR_GREEN" "$CLR_RESET" "$final_msg"
    else
        printf "\r%b[✖]%b %s\n" "$CLR_RED" "$CLR_RESET" "$final_msg"
    fi
}

# wget-style progress "spinner with percent" (more discreet)
# We will print lines like: [▌▌▌-----] 34% 1.2MiB/s ETA 00:01:23 Save to: /path
# Implementation will be used in Part 2 when invoking wget; helper here to draw line.
draw_wget_progress_line() {
    # args: percent speed eta dest
    local percent=${1:-0}
    local speed=${2:-0}
    local eta=${3:---:--}
    local dest=${4:-}
    # build bar (10 chars)
    local bars=$((percent / 10))
    local barstr=""
    for ((i=0;i<bars;i++)); do barstr+="▌"; done
    for ((i=bars;i<10;i++)); do barstr+="-"; done
    printf "\r%b[%s]%b %3s%% %s/s ETA %s Save to: %s" "$CLR_GREEN" "$barstr" "$CLR_RESET" "$percent" "$speed" "$eta" "$dest"
}

# ---------------------------
# DEPENDENCY CHECK (curl,wget,git,rsync,gpg optional)
# - Prefer curl; fallback to wget automatically in handlers.
# ---------------------------
check_dependencies() {
    local miss=()
    for cmd in bash tar xz gzip sed awk grep stat; do
        command -v "$cmd" >/dev/null 2>&1 || miss+=("$cmd")
    done
    # network tools recommended, not strictly fatal (we can fallback)
    local recommend=()
    for cmd in curl git wget rsync gpg; do
        command -v "$cmd" >/dev/null 2>&1 || recommend+=("$cmd")
    done
    if ((${#miss[@]})); then
        printf "%b%s%b %bERROR:%b Missing required commands: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "${miss[*]}"
        exit 1
    fi
    if ((${#recommend[@]})); then
        printf "%b%s%b %bWARNING:%b Optional network tools missing: %s (fetch will try fallbacks)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "${recommend[*]}" >>"$GLOBAL_LOG"
    fi
}

# ---------------------------
# SAFE TEMP DIR AND CLEANUP
# ---------------------------
__TMP_DIR_CREATED=0
make_tmpdir() {
    if [[ -z "${ADM_FETCH_TMPDIR:-}" ]]; then
        ADM_FETCH_TMPDIR=$(mktemp -d "${TMP_DIR%/}/adm-fetch.XXXXXX")
        __TMP_DIR_CREATED=1
    fi
    echo "$ADM_FETCH_TMPDIR"
}

cleanup_tmpdir() {
    if [[ "$__TMP_DIR_CREATED" -eq 1 && -n "${ADM_FETCH_TMPDIR:-}" && -d "$ADM_FETCH_TMPDIR" ]]; then
        rm -rf "$ADM_FETCH_TMPDIR" || true
    fi
}

# ensure cleanup on exit
trap 'cleanup_tmpdir' EXIT INT TERM

# ---------------------------
# PARSING sources.list (skeleton)
# format: pkg_id | version | url | sha256 | build-deps | type
# ---------------------------
declare -A SRC_VERSION SRC_URL SRC_SHA256 SRC_DEPS SRC_TYPE

load_sources_list() {
    if [[ ! -f "$SOURCES_LIST" ]]; then
        printf "%b%s%b %bERROR:%b sources.list not found: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$SOURCES_LIST"
        exit 1
    fi
    while IFS= read -r line || [[ -n "$line" ]]; do
        # strip BOM and Windows CR
        line="${line//$'\r'/}"
        line="${line//$'\xef\xbb\xbf'/}"
        # skip comments and empty lines
        [[ -z "${line//[[:space:]]/}" ]] && continue
        [[ "${line##[[:space:]]#}" != "$line" ]] && continue
        # split
        IFS="$DELIM" read -r pkg ver url sha deps typ <<<"$line" || continue
        pkg="${pkg//[[:space:]]/}"
        [[ -z "$pkg" ]] && continue
        SRC_VERSION["$pkg"]="${ver:-}"
        SRC_URL["$pkg"]="${url:-}"
        SRC_SHA256["$pkg"]="${sha:-}"
        SRC_DEPS["$pkg"]="${deps:-}"
        SRC_TYPE["$pkg"]="${typ:-}"
    done < "$SOURCES_LIST"
}

# ---------------------------
# UTIL: human-readable bytes (improved)
# ---------------------------
human_readable_bytes() {
    local bytes=${1:-0}
    local -a units=(B KiB MiB GiB TiB)
    local i=0
    local val="$bytes"
    while [[ $val -ge 1024 && $i -lt 4 ]]; do
        val=$((val/1024))
        i=$((i+1))
    done
    printf "%s%s" "$val" "${units[$i]}"
}

# ---------------------------
# UTIL: safe move with fallback (atomic-ish)
# ---------------------------
safe_move() {
    # args: src dest
    local src=$1 dest=$2
    # ensure dest dir exists
    mkdir -p "$(dirname "$dest")"
    mv -f "$src" "$dest"
}

# ---------------------------
# GIT REF PARSING HELPERS (skeleton)
# - Recognize #branch=, #tag=, #commit=, or just #ref (branch/tag heuristic)
# - Implemented in Part 2 using these helpers.
# ---------------------------
parse_git_ref() {
    # args: url
    # returns: echo "repo_url|ref_type|ref_value"
    local url="$1"
    local repo="$url"
    local reftype="" refval=""
    if [[ "$url" == *"#"* ]]; then
        repo="${url%%#*}"
        local frag="${url#*#}"
        # allow multiple forms: commit=<hash>, tag=<tag>, branch=<branch>, or just <ref>
        if [[ "$frag" =~ ^commit=([0-9a-fA-F]+)$ ]]; then
            reftype="commit"; refval="${BASH_REMATCH[1]}"
        elif [[ "$frag" =~ ^tag=(.+)$ ]]; then
            reftype="tag"; refval="${BASH_REMATCH[1]}"
        elif [[ "$frag" =~ ^branch=(.+)$ ]]; then
            reftype="branch"; refval="${BASH_REMATCH[1]}"
        else
            # ambiguous: prefer tag if starts with 'v' and contains digits, else branch
            if [[ "$frag" =~ ^v[0-9] ]] ; then
                reftype="tag"; refval="$frag"
            else
                reftype="branch"; refval="$frag"
            fi
        fi
    fi
    printf "%s|%s|%s" "$repo" "$reftype" "$refval"
}

# ---------------------------
# HOOK RUNNER (global + per-pkg)
# ---------------------------
run_hooks() {
    # args: phase (pre|post) area (fetch) pkg(optional)
    local phase="$1"; shift || true
    local area="$1"; shift || true
    local pkg="${1:-}"
    local hook_base

    # global hooks for area
    hook_base="$HOOK_DIR/$phase/$area"
    if [[ -d "$hook_base" ]]; then
        for h in "$hook_base"/*; do
            [[ -x "$h" ]] || continue
            printf "%b%s%b %b=>%b Running hook: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$h"
            if ! "$h" "$ADM_ROOT" "$pkg" >>"$GLOBAL_LOG" 2>&1; then
                printf "%b%s%b %bWARNING:%b Hook failed: %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$h" "$GLOBAL_LOG"
                return 1
            fi
        done
    fi

    # package-specific hooks
    if [[ -n "$pkg" ]]; then
        hook_base="$HOOK_DIR/$phase/$pkg"
        if [[ -d "$hook_base" ]]; then
            for h in "$hook_base"/*; do
                [[ -x "$h" ]] || continue
                printf "%b%s%b %b=>%b Running hook: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$h"
                if ! "$h" "$ADM_ROOT" "$pkg" >>"$GLOBAL_LOG" 2>&1; then
                    printf "%b%s%b %bWARNING:%b Hook failed: %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$h" "$GLOBAL_LOG"
                    return 1
                fi
            done
        fi
    fi

    return 0
}

# ---------------------------
# CURL & WGET WRAPPERS (with retry and progress)
# - try curl first, fallback to wget if curl not present or fails
# - each attempt logs verbose output to pkglog and quiet short lines to terminal
# ---------------------------
http_download_curl() {
    # args: url dest pkg pkglog
    local url="$1" dest="$2" pkg="$3" pkglog="$4"
    local attempt=1
    local status=1
    while [[ $attempt -le $NET_RETRIES ]]; do
        # remove partial from prior attempts
        rm -f "${dest}.part" 2>/dev/null || true

        # spawn curl with progress-meter disabled (we'll do our own polling)
        # -L follow redirects, --fail to get non-zero on HTTP errors
        # write to .part and move atomically
        printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "Downloading (curl): $url"
        if curl -L --fail --retry 2 --retry-delay 2 -o "${dest}.part" "$url" >>"$pkglog" 2>&1 & then
            local cpid=$!
            # polling progress
            local startt=$(date +%s)
            local last_size=0
            local clen
            clen=$(curl -sI "$url" | awk '/[Cc]ontent-Length/ {print $2}' | tr -d '\r' || true)
            while kill -0 "$cpid" >/dev/null 2>&1; do
                sleep 0.5
                local cur=$(stat -c%s "${dest}.part" 2>/dev/null || echo 0)
                local elapsed=$(( $(date +%s) - startt ))
                local speed=0
                (( elapsed > 0 )) && speed=$(( cur / elapsed ))
                if [[ -n "$clen" && "$clen" -gt 0 ]]; then
                    local pct=$(( cur * 100 / clen ))
                    # show compact progress line
                    printf "\r%b %s %b %3s%% %s saved Save to: %s" "$CLR_GREEN" "${__spinner_frames[$(( RANDOM % ${#__spinner_frames[@]} ))]}" "$CLR_RESET" "$pct" "$(human_readable_bytes $speed)" "$dest"
                else
                    printf "\r%b %s %b %s saved Save to: %s" "$CLR_GREEN" "${__spinner_frames[$(( RANDOM % ${#__spinner_frames[@]} ))]}" "$CLR_RESET" "$(human_readable_bytes $cur)" "$dest"
                fi
            done
            wait "$cpid"
            status=$?
            printf "\n" # newline after progress
        else
            status=1
        fi

        if [[ $status -eq 0 ]]; then
            safe_move "${dest}.part" "$dest"
            printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "Downloaded: $dest"
            return 0
        else
            printf "%b%s%b %bWARNING:%b curl attempt %d failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$attempt" "$url" "$pkglog"
            attempt=$((attempt+1))
            sleep $NET_RETRY_DELAY
        fi
    done
    return 1
}

http_download_wget() {
    # args: url dest pkg pkglog
    local url="$1" dest="$2" pkg="$3" pkglog="$4"
    local attempt=1
    local status=1
    while [[ $attempt -le $NET_RETRIES ]]; do
        rm -f "${dest}.part" 2>/dev/null || true
        printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "Downloading (wget): $url"
        # --show-progress prints a progress bar but to stderr; we'll parse file size instead
        if wget --tries=2 --wait=1 -O "${dest}.part" "$url" >>"$pkglog" 2>&1 & then
            local wpid=$!
            local startt=$(date +%s)
            while kill -0 "$wpid" >/dev/null 2>&1; do
                # build a modest wget-style progress (percent estimated by stat+content-length)
                sleep 0.5
                local cur=$(stat -c%s "${dest}.part" 2>/dev/null || echo 0)
                # try to get content-length via HEAD
                local clen=$(curl -sI "$url" | awk '/[Cc]ontent-Length/ {print $2}' | tr -d '\r' || true)
                if [[ -n "$clen" && "$clen" -gt 0 ]]; then
                    local pct=$(( cur * 100 / clen ))
                    # speed estimate
                    local elapsed=$(( $(date +%s) - startt ))
                    local speed=0
                    (( elapsed > 0 )) && speed=$(( cur / elapsed ))
                    draw_wget_progress_line "$pct" "$(human_readable_bytes $speed)" "--:--" "$dest"
                else
                    printf "\r%b %s %b %s saved Save to: %s" "$CLR_GREEN" "${__spinner_frames[$(( RANDOM % ${#__spinner_frames[@]} ))]}" "$CLR_RESET" "$(human_readable_bytes $cur)" "$dest"
                fi
            done
            wait "$wpid"
            status=$?
            printf "\n"
        else
            status=1
        fi

        if [[ $status -eq 0 ]]; then
            safe_move "${dest}.part" "$dest"
            printf "%b%s%b %b=>%b %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "Downloaded (wget): $dest"
            return 0
        else
            printf "%b%s%b %bWARNING:%b wget attempt %d failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$attempt" "$url" "$pkglog"
            attempt=$((attempt+1))
            sleep $NET_RETRY_DELAY
        fi
    done
    return 1
}

# wrapper that chooses curl or wget automatically
http_download() {
    # args: url dest pkg pkglog
    local url="$1" dest="$2" pkg="$3" pkglog="$4"
    if command -v curl >/dev/null 2>&1; then
        if http_download_curl "$url" "$dest" "$pkg" "$pkglog"; then
            return 0
        else
            printf "%b%s%b %b=>%b curl failed, falling back to wget\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET"
        fi
    fi
    if command -v wget >/dev/null 2>&1; then
        if http_download_wget "$url" "$dest" "$pkg" "$pkglog"; then
            return 0
        fi
    fi
    return 1
}

# ---------------------------
# GIT HANDLER: clone, checkout ref, archive to tar.gz, cleanup
# - Uses parse_git_ref from Part1 which returns repo|type|ref
# - Supports shallow clones for branches/tags, full clone for commit if needed
# ---------------------------
git_fetch_and_archive() {
    # args: url dest pkg pkglog
    local url="$1" dest="$2" pkg="$3" pkglog="$4"
    local parsed
    parsed=$(parse_git_ref "$url")
    local repo="${parsed%%|*}"
    local rest="${parsed#*|}"
    local reftype="${rest%%|*}"
    local refval="${rest#*|}"
    local tmpdir
    tmpdir=$(mktemp -d "${TMP_DIR%/}/adm-git.XXXX") || return 1
    local attempt=1
    local status=1

    while [[ $attempt -le $GIT_RETRIES ]]; do
        rm -rf "$tmpdir" || true
        mkdir -p "$tmpdir"
        printf "%b%s%b %b=>%b Cloning %s (attempt %d)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$repo" "$attempt"
        if [[ -n "$reftype" ]]; then
            # attempt shallow clone of branch or tag
            if [[ "$reftype" == "branch" || "$reftype" == "tag" ]]; then
                if git clone --depth 1 --branch "$refval" "$repo" "$tmpdir" >>"$pkglog" 2>&1; then
                    status=0
                else
                    printf "%b%s%b %bWARNING:%b shallow clone failed for %s#%s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$repo" "$refval"
                    # fallback to full clone for resilience
                    if git clone "$repo" "$tmpdir" >>"$pkglog" 2>&1; then
                        status=0
                        # try checkout
                        (cd "$tmpdir" && git checkout "$refval") >>"$pkglog" 2>&1 || status=1
                    else
                        status=1
                    fi
                fi
            elif [[ "$reftype" == "commit" ]]; then
                # need full clone or partial fetch
                if git clone --no-checkout "$repo" "$tmpdir" >>"$pkglog" 2>&1; then
                    (cd "$tmpdir" && git fetch --depth=1 origin "$refval" && git checkout FETCH_HEAD) >>"$pkglog" 2>&1 && status=0 || status=1
                else
                    status=1
                fi
            fi
        else
            # no ref provided: shallow clone default branch
            if git clone --depth 1 "$repo" "$tmpdir" >>"$pkglog" 2>&1; then
                status=0
            else
                # fallback to full clone
                if git clone "$repo" "$tmpdir" >>"$pkglog" 2>&1; then
                    status=0
                else
                    status=1
                fi
            fi
        fi

        if [[ $status -eq 0 ]]; then
            # get commit short
            pushd "$tmpdir" >/dev/null 2>&1 || true
            local commit
            commit=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")
            popd >/dev/null 2>&1 || true
            # create tarball
            mkdir -p "$(dirname "$dest")"
            tar -C "$tmpdir" -czf "$dest" . >>"$pkglog" 2>&1 || { status=1; }
            if [[ $status -eq 0 ]]; then
                printf "%b%s%b %b=>%b Git archive created: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$dest"
                # cleanup clone
                rm -rf "$tmpdir"
                return 0
            fi
        fi

        attempt=$((attempt+1))
        sleep $GIT_RETRY_DELAY
    done

    # cleanup on failure
    rm -rf "$tmpdir" || true
    printf "%b%s%b %bERROR:%b git fetch failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$repo" "$pkglog"
    return 1
}

# ---------------------------
# RSYNC handler
# ---------------------------
rsync_fetch() {
    # args: url dest pkg pkglog
    local url="$1" dest="$2" pkg="$3" pkglog="$4"
    mkdir -p "$(dirname "$dest")"
    printf "%b%s%b %b=>%b rsyncing %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$url"
    # rsync to a temporary directory then tar if it's a dir
    local tmpd
    tmpd=$(mktemp -d "${TMP_DIR%/}/adm-rsync.XXXX") || return 1
    if rsync -a --partial --stats "$url" "$tmpd" >>"$pkglog" 2>&1; then
        # check if result is single file or directory
        local entries
        entries=$(ls -A "$tmpd" 2>/dev/null | wc -l || echo 0)
        if [[ $entries -eq 1 ]]; then
            # move the one file
            local single
            single=$(ls -A "$tmpd" | head -n1)
            safe_move "$tmpd/$single" "$dest"
        else
            # create tarball
            tar -C "$tmpd" -czf "$dest" . >>"$pkglog" 2>&1 || { rm -rf "$tmpd"; return 1; }
        fi
        rm -rf "$tmpd"
        printf "%b%s%b %b=>%b rsync saved to %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$dest"
        return 0
    else
        rm -rf "$tmpd"
        printf "%b%s%b %bERROR:%b rsync failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$url" "$pkglog"
        return 1
    fi
}

# ---------------------------
# SHA256 and GPG verification
# ---------------------------
verify_sha256() {
    # args: file expected_sha pkg pkglog
    local file="$1" expected="$2" pkg="$3" pkglog="$4"
    if [[ -z "$expected" || "$expected" == "-" ]]; then
        # nothing to verify
        return 0
    fi
    if ! command -v sha256sum >/dev/null 2>&1; then
        printf "%b%s%b %bWARNING:%b sha256sum not available; skipping verification for %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$file" >>"$pkglog"
        return 0
    fi
    local got
    got=$(sha256sum "$file" 2>>"$pkglog" | awk '{print $1}') || { printf "%b%s%b %bERROR:%b sha256 failed to read %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$file" >>"$pkglog"; return 1; }
    if [[ "$got" == "$expected" ]]; then
        printf "%b%s%b %b=>%b SHA256 OK: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$file" >>"$pkglog"
        return 0
    else
        printf "%b%s%b %bERROR:%b SHA256 mismatch for %s (expected %s got %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$file" "$expected" "$got" >>"$pkglog"
        return 1
    fi
}

verify_gpg_signature() {
    # args: file url pkg pkglog
    local file="$1" url="$2" pkg="$3" pkglog="$4"
    # check for .asc or .sig next to url (remote), attempt to download signature to tmp and verify
    local asc_candidates=("${url}.asc" "${url}.sig")
    local found=0
    for asc in "${asc_candidates[@]}"; do
        # use curl/wget --head to check existence
        if command -v curl >/dev/null 2>&1; then
            if curl -sI "$asc" | head -n1 | grep -qi 'HTTP/'; then
                if curl -sSfL "$asc" -o "${TMP_DIR%/}/$(basename "$asc")" >>"$pkglog" 2>&1; then
                    found=1
                    local sigfile="${TMP_DIR%/}/$(basename "$asc")"
                    break
                fi
            fi
        elif command -v wget >/dev/null 2>&1; then
            if wget --spider "$asc" >>"$pkglog" 2>&1; then
                if wget -q -O "${TMP_DIR%/}/$(basename "$asc")" "$asc" >>"$pkglog" 2>&1; then
                    found=1
                    local sigfile="${TMP_DIR%/}/$(basename "$asc")"
                    break
                fi
            fi
        fi
    done

    if [[ $found -eq 0 ]]; then
        printf "%b%s%b %bINFO:%b No signature found for %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$file" >>"$pkglog"
        return 0
    fi

    if ! command -v gpg >/dev/null 2>&1; then
        printf "%b%s%b %bWARNING:%b gpg not present; cannot verify signature for %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$file" >>"$pkglog"
        return 0
    fi

    # import keys from cfg/gpgkeys if present
    if [[ -d "$CFG_DIR/gpgkeys" ]]; then
        for k in "$CFG_DIR/gpgkeys"/*; do
            [[ -f "$k" ]] || continue
            gpg --import "$k" >>"$pkglog" 2>&1 || true
        done
    fi

    if gpg --verify "$sigfile" "$file" >>"$pkglog" 2>&1; then
        printf "%b%s%b %b=>%b GPG OK for %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_MAGENTA" "$CLR_RESET" "$file" >>"$pkglog"
        rm -f "$sigfile" || true
        return 0
    else
        printf "%b%s%b %bERROR:%b GPG verification failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$file" "$pkglog" >>"$pkglog"
        return 1
    fi
}

# ---------------------------
# High-level fetch dispatcher used by Part 3
# - Decides protocol and calls proper handler
# ---------------------------
fetch_dispatcher() {
    # args: pkg
    local pkg="$1"
    local url="${SRC_URL[$pkg]:-}"
    local sha="${SRC_SHA256[$pkg]:-}"
    local ver="${SRC_VERSION[$pkg]:-}"
    local pkglog="$LOG_DIR/${pkg}.fetch.out"
    local pkgeerr="$LOG_DIR/${pkg}.fetch.err"
    : > "$pkglog"
    : > "$pkgeerr"

    # run pre-fetch hooks
    if ! run_hooks pre fetch "$pkg"; then
        printf "%b%s%b %bWARNING:%b pre-fetch hooks failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$pkg" "$GLOBAL_LOG"
        # continue, unless hook should abort by exit non-zero - design decision: warn and continue
    fi

    # determine dest filename base
    local filename
    filename=$(basename "${url%%#*}")
    local dest="$SRC_DIR/$filename"

    # special handling for Git refs: produce <pkg>-<commit>.git.tar.gz
    if [[ "$url" =~ (\.git$|^git@|^git://) || "$url" == *"#"* && ( "$url" == *.git* || "$url" == *git* ) ]]; then
        # compute final dest: pkg-<commit>.git.tar.gz in tmp then move in place
        dest="$SRC_DIR/${pkg}-${ver:-unknown}.git.tar.gz"
        if git_fetch_and_archive "$url" "$dest" "$pkg" "$pkglog"; then
            : # continue to verify
        else
            printf "%b%s%b %bERROR:%b git fetch failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$pkg" "$pkglog"
            return 1
        fi
    elif [[ "$url" =~ ^(http|https|ftp):// ]]; then
        # standard http(s)/ftp
        if http_download "$url" "$dest" "$pkg" "$pkglog"; then
            :
        else
            printf "%b%s%b %bERROR:%b download failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$pkg" "$pkglog"
            return 1
        fi
    elif [[ "$url" =~ ^rsync:// || "$url" =~ :/ ]]; then
        if rsync_fetch "$url" "$dest" "$pkg" "$pkglog"; then
            :
        else
            printf "%b%s%b %bERROR:%b rsync failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$pkg" "$pkglog"
            return 1
        fi
    else
        printf "%b%s%b %bERROR:%b Unknown URL scheme for %s: %s\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$pkg" "$url"
        return 1
    fi

    # verify sha256 if provided
    if [[ -n "$sha" && "$sha" != "-" ]]; then
        if ! verify_sha256 "$dest" "$sha" "$pkg" "$pkglog"; then
            printf "%b%s%b %bERROR:%b SHA256 verification failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$dest" "$pkglog"
            return 1
        fi
    fi

    # verify gpg signature if present
    if ! verify_gpg_signature "$dest" "$url" "$pkg" "$pkglog"; then
        printf "%b%s%b %bERROR:%b GPG verification failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$dest" "$pkglog"
        return 1
    fi

    # run post-fetch hooks
    if ! run_hooks post fetch "$pkg"; then
        printf "%b%s%b %bWARNING:%b post-fetch hooks failed for %s (see %s)\n" "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_YELLOW" "$CLR_RESET" "$pkg" "$GLOBAL_LOG"
        # continue
    fi

    # record success in GLOBAL_LOG
    printf "%s %s %s\n" "$(date '+%Y-%m-%d %H:%M:%S')" "OK" "$pkg" >>"$GLOBAL_LOG"
    return 0
}

usage() {
    cat <<EOF
Usage: $(basename "$0") <pkg|all> [options]

Options:
  --no-verify     Skip SHA256/GPG verification
  --force         Force re-download even if cache is valid
  --help          Show this help message

Examples:
  adm-fetch bash
  adm-fetch all
  adm-fetch glibc --no-verify
EOF
    exit 0
}

NO_VERIFY=0
FORCE_REDOWNLOAD=0
if [[ $# -lt 1 ]]; then
    usage
fi

ARG="$1"
shift || true
while [[ $# -gt 0 ]]; do
    case "$1" in
        --no-verify) NO_VERIFY=1 ;;
        --force) FORCE_REDOWNLOAD=1 ;;
        --help|-h) usage ;;
    esac
    shift
done

check_dependencies
load_sources_list

headline "Starting adm-fetch v2.0"

if [[ "$ARG" == "all" ]]; then
    pkgs=("${!SRC_URL[@]}")
else
    if [[ -z "${SRC_URL[$ARG]:-}" ]]; then
        printf "%b%s%b %bERROR:%b package not found in sources.list: %s\n" \
            "$CLR_CYAN" "$(timestamp)" "$CLR_RESET" "$CLR_RED" "$CLR_RESET" "$ARG"
        exit 1
    fi
    pkgs=("$ARG")
fi

overall_fail=0
total=${#pkgs[@]}
count=1

for pkg in "${pkgs[@]}"; do
    local_url="${SRC_URL[$pkg]}"
    local_sha="${SRC_SHA256[$pkg]}"
    local_ver="${SRC_VERSION[$pkg]}"
    local_filename="$(basename "${local_url%%#*}")"
    local_dest="$SRC_DIR/$local_filename"

    headline "[$count/$total] Fetching $pkg"
    count=$((count+1))

    # Skip cache check for git or rsync sources
    if [[ "$local_url" =~ (\.git$|^git@|^git://|#) || "$local_url" =~ ^rsync:// ]]; then
        printf "%b[%s]%b %s\n" "$CLR_GREEN" "$(timestamp)" "$CLR_RESET" "Non-cacheable source type (git/rsync)"
    else
        # Cache validation
        if [[ -f "$local_dest" && "$FORCE_REDOWNLOAD" -eq 0 && "$NO_VERIFY" -eq 0 ]]; then
            if verify_sha256 "$local_dest" "$local_sha" "$pkg" "$LOG_DIR/${pkg}.fetch.out"; then
                printf "%b[%s]%b %s\n" "$CLR_GREEN" "$(timestamp)" "$CLR_RESET" "Cached OK: $local_dest (skipped)"
                continue
            else
                printf "%b[%s]%b %s\n" "$CLR_YELLOW" "$(timestamp)" "$CLR_RESET" "Checksum mismatch, re-downloading..."
                rm -f "$local_dest" || true
            fi
        elif [[ "$FORCE_REDOWNLOAD" -eq 1 ]]; then
            printf "%b[%s]%b %s\n" "$CLR_YELLOW" "$(timestamp)" "$CLR_RESET" "Forced re-download for $pkg"
            rm -f "$local_dest" || true
        fi
    fi

    # Spinner start
    start_spinner "Fetching package $pkg"
    if fetch_dispatcher "$pkg" >>"$LOG_DIR/${pkg}.fetch.out" 2>>"$LOG_DIR/${pkg}.fetch.err"; then
        stop_spinner 0 "Fetched: $pkg"
        printf "%b[%s]%b %s\n" "$CLR_GREEN" "$(timestamp)" "$CLR_RESET" "Saved to: $SRC_DIR/$(basename "${SRC_URL[$pkg]%%#*}")"
    else
        stop_spinner 1 "Failed: $pkg (see $LOG_DIR/${pkg}.fetch.err)"
        overall_fail=1
    fi
done

# Final summary
echo
if [[ "$overall_fail" -eq 0 ]]; then
    echo -e "${CLR_GREEN}All fetches completed successfully.${CLR_RESET}"
else
    echo -e "${CLR_RED}Some fetches failed. See logs in $LOG_DIR/*.fetch.err${CLR_RESET}"
fi
echo -e "${CLR_MAGENTA}Logs written to:${CLR_RESET} ${CLR_CYAN}$GLOBAL_LOG${CLR_RESET}"
echo -e "${CLR_MAGENTA}Sources cached at:${CLR_RESET} ${CLR_CYAN}$SRC_DIR${CLR_RESET}"
echo

exit "$overall_fail"
